// src/app/api/chat/route.ts

import { NextRequest, NextResponse } from "next/server";
import { GoogleGenerativeAI } from "@google/generative-ai";
import { createServerClient } from "@supabase/ssr";
import { CURRENT_LLM_PROVIDER, LLM_MODELS } from "@/lib/llm-config";
import * as GroqClient from "@/lib/groq-client";

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);

export async function POST(request: NextRequest) {
  const supabase = createServerClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
    {
      cookies: {
        get(name: string) {
          return request.cookies.get(name)?.value;
        },
      },
    }
  );

  const {
    data: { user },
  } = await supabase.auth.getUser();

  if (!user) {
    return NextResponse.json({ error: "No autorizado" }, { status: 401 });
  }

  try {
    const { message, plantId, chatHistory } = await request.json();

    if (!message || !plantId) {
      return NextResponse.json(
        { error: "Faltan datos requeridos" },
        { status: 400 }
      );
    }

    // Obtener informaciÃ³n completa de la planta
    const { data: plant, error: plantError } = await supabase
      .from("plants")
      .select("*")
      .eq("id", plantId)
      .eq("user_id", user.id)
      .single();

    if (plantError || !plant) {
      return NextResponse.json(
        { error: "Planta no encontrada" },
        { status: 404 }
      );
    }

    // Construir el contexto de la planta
    const plantContext = `
Eres un experto botÃ¡nico y jardinero profesional especializado en el cuidado de plantas. EstÃ¡s ayudando a un usuario con su planta especÃ­fica.

INFORMACIÃ“N DE LA PLANTA DEL USUARIO:
- Nombre: ${plant.name}
- Nivel de Cuidado: ${plant.care_level || "No especificado"}
- Apta para mascotas: ${plant.pet_friendly ? "SÃ­" : "No"}
- TÃ³xica: ${plant.is_toxic ? "SÃ­" : "No"}
- Fecha de registro: ${new Date(plant.created_at).toLocaleDateString("es-ES")}

GUÃA DE CUIDADOS ACTUAL:
${plant.care_instructions}

INSTRUCCIONES PARA TI:
1. Responde de forma clara, amigable y personalizada
2. Usa la informaciÃ³n de la planta para dar consejos especÃ­ficos
3. Si el usuario pregunta algo que ya estÃ¡ en la guÃ­a de cuidados, referencia esa informaciÃ³n
4. Si la pregunta es sobre sÃ­ntomas (hojas amarillas, manchas, etc.), sÃ© especÃ­fico en el diagnÃ³stico
5. Proporciona soluciones prÃ¡cticas y fÃ¡ciles de implementar
6. Si la pregunta no estÃ¡ relacionada con plantas o jardinerÃ­a, gentilmente redirige al usuario
7. MantÃ©n las respuestas concisas pero completas (mÃ¡ximo 250 palabras)
8. Usa emojis ocasionalmente para hacer la conversaciÃ³n mÃ¡s amena ðŸŒ¿
9. Si no estÃ¡s seguro de algo, admÃ­telo y sugiere consultar con un experto local
10. IMPORTANTE: Usa formato de texto simple. Si necesitas resaltar algo importante, usa texto en MAYÃšSCULAS o emojis destacados, pero evita usar asteriscos ** o sÃ­mbolos de markdown

CONTEXTO ADICIONAL:
El usuario estÃ¡ en Guatemala, con clima templado a subtropical.
`;

    let responseText: string;

    if (CURRENT_LLM_PROVIDER === "groq") {
      // Usar Groq
      const messages = [
        { role: "user" as const, content: plantContext },
        {
          role: "assistant" as const,
          content: `Â¡Entendido! Estoy listo para ayudarte con tu ${plant.name}. Tengo toda la informaciÃ³n sobre sus cuidados y caracterÃ­sticas. Â¿QuÃ© te gustarÃ­a saber? ðŸŒ±`,
        },
        ...(chatHistory || []).map((msg: any) => ({
          role: (msg.role === "user" ? "user" : "assistant") as
            | "user"
            | "assistant",
          content: msg.content,
        })),
        { role: "user" as const, content: message },
      ];

      responseText = await GroqClient.generateChatResponse(
        LLM_MODELS.groq,
        messages
      );
    } else {
      // Usar Gemini
      const model = genAI.getGenerativeModel({ model: LLM_MODELS.gemini });

      const history = chatHistory
        ? chatHistory.map((msg: any) => ({
            role: msg.role === "user" ? "user" : "model",
            parts: [{ text: msg.content }],
          }))
        : [];

      const chat = model.startChat({
        history: [
          {
            role: "user",
            parts: [{ text: plantContext }],
          },
          {
            role: "model",
            parts: [
              {
                text: `Â¡Entendido! Estoy listo para ayudarte con tu ${plant.name}. Tengo toda la informaciÃ³n sobre sus cuidados y caracterÃ­sticas. Â¿QuÃ© te gustarÃ­a saber? ðŸŒ±`,
              },
            ],
          },
          ...history,
        ],
      });

      const result = await chat.sendMessage(message);
      const response = await result.response;
      responseText = response.text();
    }

    return NextResponse.json({
      response: responseText,
      plantName: plant.name,
      provider: CURRENT_LLM_PROVIDER,
    });
  } catch (error) {
    console.error("Error en el chat:", error);
    return NextResponse.json(
      {
        error:
          error instanceof Error ? error.message : "Error interno del servidor",
      },
      { status: 500 }
    );
  }
}
